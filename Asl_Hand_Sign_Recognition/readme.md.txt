Real-Time American Sign Language (ASL) Alphabet Recognition

Using 21-Point Hand Landmarks and LSTM-Based Temporal Modeling

Abstract

American Sign Language (ASL) serves as a critical medium of communication for the hearing-impaired community. Traditional vision-based sign recognition systems often rely on raw images or videos, leading to high computational cost and sensitivity to background, lighting, and camera variations.

This work presents a real-time ASL alphabet recognition system (A–Z) based on hand landmark sequences rather than raw images. Using 21 three-dimensional hand landmarks per frame, temporal gesture dynamics are modeled via a multi-layer Long Short-Term Memory (LSTM) network. The system performs real-time inference from a webcam, constructs words from stable alphabet predictions, and converts recognized text into English (offline) and Hindi (online) speech output.

Keywords

American Sign Language, Hand Landmarks, LSTM, Sequence Modeling, Human–Computer Interaction, Assistive Technology

1. Introduction

Sign language recognition is an important research area in computer vision and human–computer interaction. Image-based deep learning approaches often require large datasets and are sensitive to environmental variations.

To address these challenges, this project adopts a landmark-based representation, where each hand gesture is encoded as a sequence of spatial keypoints. This representation significantly reduces input dimensionality while preserving essential gesture information.

The proposed system focuses on:

Robust real-time recognition

Lightweight computation

Ease of deployment on consumer hardware

2. System Architecture

Overall Pipeline :-

Webcam Input
   ↓
Hand Detection & Tracking (MediaPipe)
   ↓
21 Hand Landmarks (x, y, z)
   ↓
Landmark Normalization
   ↓
Temporal Sequence Buffer (16 frames)
   ↓
LSTM-Based Classifier
   ↓
Alphabet Prediction (A–Z)
   ↓
Stability Filtering
   ↓
Word Formation
   ↓
Speech Output (English + Hindi)

3. Landmark Representation

Each video frame is represented using 21 hand landmarks, where each landmark contains:

x: normalized horizontal coordinate

y: normalized vertical coordinate

z: relative depth

Feature Vector

Per frame: 21 × 3 = 63 features

Per sample (gesture): 16 frames

Final input tensor:

(1, 16, 63)

Normalization Strategy

Wrist landmark is used as the origin

Scale normalization is performed using palm landmark distance

Ensures invariance to hand size and camera distance

4. Dataset Description

A custom ASL landmark dataset was created by recording hand gestures and extracting landmarks using MediaPipe.

Dataset Characteristics

File format: .npy

One file per gesture instance

Shape: (16, 21, 3)

Classes: 26 (A–Z)

Directory Structure
dataset/landmarks/
├── A/
├── B/
├── C/
└── ...


This dataset is fully generated by the author and does not rely on external landmark datasets.

5. Model Architecture
LSTM-Based Temporal Model

The temporal evolution of hand gestures is modeled using a two-layer LSTM network.

Input Sequence (16 × 63)
   ↓
LSTM (2 layers, hidden size = 256)
   ↓
Last Time-Step Output
   ↓
Fully Connected Layer
   ↓
Softmax (26 classes)

Implementation Details

Framework: PyTorch

Loss Function: Cross-Entropy Loss

Inference: Softmax probability with confidence thresholding

6. Stability Filtering and Word Formation

To ensure reliable predictions in real-time conditions, a stability-based decision mechanism is applied:

A letter is accepted only after being predicted consistently for multiple frames

Once confirmed, the system locks the letter

The user removes the hand to proceed to the next letter

Letters are concatenated to form complete words

This design significantly reduces prediction flicker and improves usability.

7. Speech Synthesis and Translation
English Speech (Offline)

Implemented using pyttsx3

No internet connection required

Hindi Speech (Online)

English word translated to Hindi using Google Translate

Hindi speech generated using Google Text-to-Speech (gTTS)

This enables multilingual assistive communication.

8. Experimental Setup
Runtime Environment

Python ≥ 3.9

CPU / GPU supported

Webcam-based input

Key Parameters
Parameter	Value
Sequence Length	16
Confidence Threshold	0.6
Stable Frames	8
No-hand Frames to Speak	20
9. Results and Observations

The system demonstrates stable real-time recognition

Landmark-based input reduces sensitivity to background and lighting

Temporal modeling improves accuracy compared to single-frame prediction

Lightweight design allows smooth execution on consumer hardware

10. Applications

Assistive communication for hearing-impaired users

Human–computer interaction systems

Educational tools for learning sign language

Real-time gesture-controlled interfaces

11. Limitations and Future Work

Current system supports isolated alphabets only

Dynamic word-level signs are not included

Future extensions may include:

Transformer-based temporal models

Continuous sign recognition

Mobile and edge deployment

Grammar-aware sentence formation

12. Conclusion

This project demonstrates an effective and efficient approach to ASL alphabet recognition using hand landmark sequences and LSTM-based temporal modeling. By avoiding raw image processing, the system achieves robustness, speed, and deployability, making it suitable for real-world assistive applications.